# @package _global_
name: 'densenet_training_100epoch'
working_dir: saved
device: 'cuda'
random_seed: ~
num_epoch: 
  enable: False
  total: 300  # max iteration
num_step:
  enable: True
  total: 40000


data:
  mode: "cifar100"
  transform: 
    cifar100: 
      mean: 
        - 0.5071
        - 0.4867
        - 0.4408
      std: 
        - 0.2675
        - 0.2565
        - 0.2761
  train_dir: 'dataset/meta/train'
  test_dir: 'dataset/meta/test'
  file_format: '*.file_extension'
  use_background_generator: true
  divide_dataset_per_gpu: true
  

num_workers: 6
batch_size: 64
model:
  type: "EfficientNet-B0"
  optimizer: 'SGD'  

  wide-resnet:
    SGD:
      lr: 0.1
      momentum: 0.9
      weight_decay: 5e-4
      nesterov: True
  densenet:
    SGD:
      lr: 0.1
      momentum: 0.9
      weight_decay: 1e-4
    SAM:
      lr: 0.1
      momentum: 0.9
  EfficientNet-B0:
    SGD:
      lr: 0.1
      momentum: 0.9
      nesterov: true
      weight_decay: 1e-4

sweep:
  enable: true
  config:
    name: 'First Sweep'
    method: 'grid'
    metric:
      name: 'test_accuracy'
      goal: 'maximize'
    parameter:
      lr:
        min: 1e-4
        max: 1e-1
      weight_decay:
        min: 1e-6
        max: 1e-3

dist:
  master_addr: 'localhost'
  master_port: '8088'
  mode: 'nccl'
  gpus: -1 # 0 for not using dist, -1 for using all gpus
  timeout: ~ # seconds for timeout. In nccl mode, set ~ for not using timeout
log:
  use_tensorboard: false
  use_wandb: true
  wandb_init_conf:
    name: ~
    entity: ~
    project: "cvlab-cifar100-efficient-net"
  summary_interval: 30 # interval of step
  chkpt_interval: 100 # interval of epoch
  chkpt_dir: 'chkpt'
load:
  wandb_load_path: ~
  network_chkpt_path: ~
  strict_load: false
  resume_state_path: ~
